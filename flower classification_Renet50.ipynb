{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Flower Classification CNN**"},{"metadata":{},"cell_type":"markdown","source":"This was a hackerearth challenge\n\nGiven a large class of flowers, 102 to be precise. Build a flower classification model which is discriminative between classes but can correctly classify all flower images belonging to the same class. There are a total of 20549 (train + test) images of flowers. Predict the category of the flowers present in the test folder with good accuracy.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Dropout ,BatchNormalization\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nfrom keras import models\n\nimport pandas as pd\nimport numpy as np\nimport time\nimport datetime\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = 40591 #number of train images\nval_images = 10101 #number of validation images\ntrain_batchsize = 50 #number of train images in each batch\nval_batchsize = 50 #number of validation images in each batch\nimg_shape=(224,224) #image shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#since the dataset is huge, we use generators to train the model\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nx_train = train_datagen.flow_from_directory(\n    directory=r'../input/flower-datatree/datatree/train/', #location of train images\n    batch_size=train_batchsize,\n    target_size=img_shape,\n    class_mode=\"categorical\", #classification \n    shuffle=True, #shuffling the train images\n    seed=42 #seed for the shuffle\n)\n\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\nx_validation = validation_datagen.flow_from_directory(\n    directory=r'../input/flower-datatree/datatree/validation/', #location of validation images\n    batch_size=val_batchsize,\n    target_size=img_shape,\n    class_mode=\"categorical\", #classification\n    shuffle=True, #shuffling the validation images\n    seed=42 #seed for the shuffle\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#building the model architecture\n\nmodel = Sequential()\n\nmodel.add(ResNet50(include_top=False, pooling='avg', weights='imagenet'))\n#model.add(Flatten())\n#model.add(BatchNormalization())\n#model.add(Dense(2048, activation='relu'))\n#model.add(BatchNormalization())\n#model.add(Dense(1024, activation='relu'))\n#model.add(BatchNormalization())\n#model.add(Dense(512, activation='relu'))\n#model.add(BatchNormalization())\nmodel.add(Dense(102, activation='softmax'))\n\n#model.layers[0].trainable = False\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc']) # optimizer=RMSprop(lr=0.001)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_steps=int(np.ceil(train_images//train_batchsize)) #number of steps for training the model\nval_steps=int(np.ceil(val_images//val_batchsize)) #number of steps for validating the model\nprint(train_steps,val_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=1, mode='auto')\n# Reducing the learning Rate if result is not improving\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', min_delta=0.0004, patience=2, factor=0.1, min_lr=1e-6, mode='auto',verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"savepath=\"flowermodel.hdf5\"\ncheckpoint = ModelCheckpoint(savepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') \n#saves the model only with the highest validation accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=time.time()\ncnn=model.fit_generator(x_train,steps_per_epoch = train_steps,validation_data=x_validation,validation_steps = val_steps,epochs=20,callbacks=[early_stop, reduce_lr , checkpoint],verbose=1)  \nend=time.time()\n\nprint('training time: '+str(datetime.timedelta(seconds=(end-start))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#accuracy\nprint(cnn.history.keys())\nplt.plot(cnn.history['acc'])\nplt.plot(cnn.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.plot(np.argmax(cnn.history[\"val_acc\"]), np.max(cnn.history[\"val_acc\"]), marker=\"x\", color=\"r\",label=\"best model\")\nplt.legend(['Training set', 'Test set','best'], loc='upper left')\nplt.show()\n\n#loss\nplt.plot(cnn.history['loss'])\nplt.plot(cnn.history['val_loss'])\nplt.title('Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training set', 'Test set'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Predciting test data using the trained model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_datagen = ImageDataGenerator(rescale=1./255)\nx_test = test_datagen.flow_from_directory(\n    directory=r'../input/flower-datatree/datatree/',\n    target_size=img_shape,\n    classes=['test'],\n    batch_size=1,\n    shuffle=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = 2009\n\ntest_stepsize = test_images\nx_test.reset() #\npredict = model.predict_generator(x_test ,steps=test_stepsize , verbose=1)\nprint(predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=[] #saving all the prediction on the test images\nfor i in predict:\n    predictions.append(np.argmax(i)+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#undoing the sorting of the categories caused by ImageDataGenerator\n####very very important####\nactual=[str(i) for i in range(1,103)]\ngen=sorted(actual)\n\nlabels={}\n\nfor i in range(1,103):\n    labels[i]=int(gen[i-1])\nn_predictions=[]\nfor i in predictions:\n    n_predictions.append(labels[i])\n\npredictions = n_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfreq=Counter()\nfreq.update(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pylab as plt\n\nlists = sorted(freq.items()) # sorted by key, return a list of tuples\nx, y = zip(*lists) # unpack a list of pairs into two tuples\nplt.figure(figsize=(20,5))\nplt.bar(x, y)\nplt.xlabel('category')\nplt.ylabel('number of images')\nplt.title(\"test results\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names=[i for i in range(18540,20549)]\nresults = pd.Series(predictions,name = \"category\")\nnames=pd.Series(names,name = \"image_id\")\nsubmission = pd.concat([names,results],axis = 1)\nsubmission.to_csv(\"output.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}